{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import loads, dumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_words</th>\n",
       "      <th>words</th>\n",
       "      <th>aspects</th>\n",
       "      <th>holder</th>\n",
       "      <th>opinions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Colorado_Technical_University_Online_2_06-11-2...</td>\n",
       "      <td>NA I am a current student of their MBA program...</td>\n",
       "      <td>[NA, I, am, a, current, student, of, their, MB...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colorado_Technical_University_Online_2_06-11-2...</td>\n",
       "      <td>NA I have been in the IT industry for 20 years...</td>\n",
       "      <td>[NA, I, have, been, in, the, IT, industry, for...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colorado_Technical_University_Online_2_06-11-2...</td>\n",
       "      <td>NA I am currently a director , and do make a s...</td>\n",
       "      <td>[NA, I, am, currently, a, director, ,, and, do...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colorado_Technical_University_Online_2_06-11-2...</td>\n",
       "      <td>NA I did my research , and am concerned about ...</td>\n",
       "      <td>[NA, I, did, my, research, ,, and, am, concern...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado_Technical_University_Online_2_06-11-2...</td>\n",
       "      <td>NA Especially , since I am planning for a PhD ...</td>\n",
       "      <td>[NA, Especially, ,, since, I, am, planning, fo...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>opener_en/kaf/hotel/english00011_0b57ac0ca8569...</td>\n",
       "      <td>NA Golf courses can be easily reached from the...</td>\n",
       "      <td>[NA, Golf, courses, can, be, easily, reached, ...</td>\n",
       "      <td>[{'index': 0, 'from': 11, 'to': 13, 'term': 't...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 17, 'to': 18, 'term': 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>opener_en/kaf/hotel/english00011_0b57ac0ca8569...</td>\n",
       "      <td>NA Hotel offers nice spa , large pool area and...</td>\n",
       "      <td>[NA, Hotel, offers, nice, spa, ,, large, pool,...</td>\n",
       "      <td>[{'index': 0, 'from': 1, 'to': 2, 'term': 'Hot...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 2, 'to': 3, 'term': 'off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>opener_en/kaf/hotel/english00011_0b57ac0ca8569...</td>\n",
       "      <td>NA Hospitality and services at the highest lev...</td>\n",
       "      <td>[NA, Hospitality, and, services, at, the, high...</td>\n",
       "      <td>[{'index': 0, 'from': 1, 'to': 2, 'term': 'Hos...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 4, 'to': 8, 'term': 'at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>opener_en/kaf/hotel/english00011_0b57ac0ca8569...</td>\n",
       "      <td>NA I will definitely come back !</td>\n",
       "      <td>[NA, I, will, definitely, come, back, !]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 1, 'to': 2, 'term': 'I'}]</td>\n",
       "      <td>[{'index': 0, 'from': 2, 'to': 7, 'term': 'wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>opener_en/kaf/hotel/english00032_30ddf6dff464d...</td>\n",
       "      <td>NA Best in class</td>\n",
       "      <td>[NA, Best, in, class]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 1, 'to': 2, 'term': 'Bes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent_id  \\\n",
       "0     Colorado_Technical_University_Online_2_06-11-2...   \n",
       "1     Colorado_Technical_University_Online_2_06-11-2...   \n",
       "2     Colorado_Technical_University_Online_2_06-11-2...   \n",
       "3     Colorado_Technical_University_Online_2_06-11-2...   \n",
       "4     Colorado_Technical_University_Online_2_06-11-2...   \n",
       "...                                                 ...   \n",
       "4370  opener_en/kaf/hotel/english00011_0b57ac0ca8569...   \n",
       "4371  opener_en/kaf/hotel/english00011_0b57ac0ca8569...   \n",
       "4372  opener_en/kaf/hotel/english00011_0b57ac0ca8569...   \n",
       "4373  opener_en/kaf/hotel/english00011_0b57ac0ca8569...   \n",
       "4374  opener_en/kaf/hotel/english00032_30ddf6dff464d...   \n",
       "\n",
       "                                              raw_words  \\\n",
       "0     NA I am a current student of their MBA program...   \n",
       "1     NA I have been in the IT industry for 20 years...   \n",
       "2     NA I am currently a director , and do make a s...   \n",
       "3     NA I did my research , and am concerned about ...   \n",
       "4     NA Especially , since I am planning for a PhD ...   \n",
       "...                                                 ...   \n",
       "4370  NA Golf courses can be easily reached from the...   \n",
       "4371  NA Hotel offers nice spa , large pool area and...   \n",
       "4372  NA Hospitality and services at the highest lev...   \n",
       "4373                   NA I will definitely come back !   \n",
       "4374                                   NA Best in class   \n",
       "\n",
       "                                                  words  \\\n",
       "0     [NA, I, am, a, current, student, of, their, MB...   \n",
       "1     [NA, I, have, been, in, the, IT, industry, for...   \n",
       "2     [NA, I, am, currently, a, director, ,, and, do...   \n",
       "3     [NA, I, did, my, research, ,, and, am, concern...   \n",
       "4     [NA, Especially, ,, since, I, am, planning, fo...   \n",
       "...                                                 ...   \n",
       "4370  [NA, Golf, courses, can, be, easily, reached, ...   \n",
       "4371  [NA, Hotel, offers, nice, spa, ,, large, pool,...   \n",
       "4372  [NA, Hospitality, and, services, at, the, high...   \n",
       "4373           [NA, I, will, definitely, come, back, !]   \n",
       "4374                              [NA, Best, in, class]   \n",
       "\n",
       "                                                aspects  \\\n",
       "0     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "1     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "2     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "3     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "...                                                 ...   \n",
       "4370  [{'index': 0, 'from': 11, 'to': 13, 'term': 't...   \n",
       "4371  [{'index': 0, 'from': 1, 'to': 2, 'term': 'Hot...   \n",
       "4372  [{'index': 0, 'from': 1, 'to': 2, 'term': 'Hos...   \n",
       "4373  [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4374  [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "\n",
       "                                                 holder  \\\n",
       "0      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "1      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "2      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "3      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "4      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "...                                                 ...   \n",
       "4370  [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4371  [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4372  [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4373    [{'index': 0, 'from': 1, 'to': 2, 'term': 'I'}]   \n",
       "4374   [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "\n",
       "                                               opinions  \n",
       "0      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "1      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "2      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "3      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "4      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "...                                                 ...  \n",
       "4370  [{'index': 0, 'from': 17, 'to': 18, 'term': 'o...  \n",
       "4371  [{'index': 0, 'from': 2, 'to': 3, 'term': 'off...  \n",
       "4372  [{'index': 0, 'from': 4, 'to': 8, 'term': 'at ...  \n",
       "4373  [{'index': 0, 'from': 2, 'to': 7, 'term': 'wil...  \n",
       "4374  [{'index': 0, 'from': 1, 'to': 2, 'term': 'Bes...  \n",
       "\n",
       "[4375 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = ['darmstadt_unis', 'mpqa', 'opener_en']\n",
    "\n",
    "df1 = [pd.read_json(f\"../../final_data/{i}/train_convert.json\") for i in datas]\n",
    "df = pd.concat(df1)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('ddx.json')\n",
    "df2 = pd.read_json('ddx1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_words</th>\n",
       "      <th>words</th>\n",
       "      <th>aspects</th>\n",
       "      <th>holder</th>\n",
       "      <th>opinions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent_id, raw_words, words, aspects, holder, opinions]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1[\"opinions\"] == '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'ddx1.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, json\n",
    "\n",
    "NULL_TOKEN = 'NA'\n",
    "START_WORD = 'NA '\n",
    "NULL_INDEX = 0\n",
    "\n",
    "def char2word_offset(sent, start_end_str):\n",
    "    start, end = [int(i) for i in start_end_str.split(':')]\n",
    "    start, end = start+len(START_WORD), end+len(START_WORD)\n",
    "    word_start, word_end = None, None\n",
    "    space_count = 0\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i]==' ': space_count += 1\n",
    "        if i>=start and word_start is None: word_start = space_count\n",
    "        if i>=end and word_start is not None: word_end = space_count; break\n",
    "    if word_end is None and word_start is not None: word_end = space_count+1\n",
    "    assert (type(word_start) == type(word_end))\n",
    "    if word_start is None: word_start, word_end = NULL_INDEX, NULL_INDEX+1\n",
    "    return int(word_start), int(word_end)\n",
    "\n",
    "def format_convert(original_data, file, threshold=0):\n",
    "    final_data = []\n",
    "    blank_cnt = 0\n",
    "    threshold *= len(original_data)\n",
    "    if 't1' in file: print(f'{threshold} limit for null opinions')\n",
    "\n",
    "    for row in tqdm(original_data):\n",
    "        data = {}\n",
    "        if 't1' in file:\n",
    "            if blank_cnt <= threshold and len(row['opinions'])==0: blank_cnt += 1\n",
    "            elif len(row['opinions'])==0: continue\n",
    "\n",
    "        row['text'] = START_WORD + row['text']\n",
    "        data['sent_id'] = row['sent_id']\n",
    "        data['raw_words'] = row['text']\n",
    "        data['words'] = data['raw_words'].split()\n",
    "        aspects, holders, opinions = [], [], []\n",
    "\n",
    "        for cnt, original_opinion in enumerate(row['opinions']):\n",
    "            aspect, holder, opinion = {}, {}, {}\n",
    "            aspect['index'], holder['index'], opinion['index'] = cnt, cnt, cnt\n",
    "\n",
    "            ############################################# Aspect #############################################\n",
    "            try:aspect['from'], aspect['to'] = char2word_offset(data['raw_words'], original_opinion['Target'][1][0])\n",
    "            except: aspect['from'], aspect['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if aspect['from'] == aspect['to']-1 == NULL_INDEX:\n",
    "                aspect['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                aspect['term'] = original_opinion['Target'][0][0]\n",
    "\n",
    "            try: aspect['Polarity'] = str(original_opinion['Polarity'])\n",
    "            except: aspect['Polarity'] = NULL_TOKEN; print(original_opinion)\n",
    "            try: aspect['Intensity'] = str(original_opinion['Intensity'])\n",
    "            except: aspect['Intensity'] = NULL_TOKEN; print(original_opinion)\n",
    "\n",
    "            ############################################# Holder #############################################\n",
    "            try:holder['from'], holder['to'] = char2word_offset(data['raw_words'], original_opinion['Source'][1][0])\n",
    "            except: holder['from'], holder['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if holder['from'] == holder['to']-1 == NULL_INDEX:\n",
    "                holder['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                holder['term']  = original_opinion['Source'][0][0]\n",
    "\n",
    "            ############################################# Opinion #############################################\n",
    "            try:opinion['from'], opinion['to'] = char2word_offset(data['raw_words'], original_opinion['Polar_expression'][1][0])\n",
    "            except: opinion['from'], opinion['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if opinion['from'] == opinion['to']-1 == NULL_INDEX:\n",
    "                opinion['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                opinion['term'] = original_opinion['Polar_expression'][0][0]\n",
    "\n",
    "            aspects.append(aspect)\n",
    "            holders.append(holder)\n",
    "            opinions.append(opinion)\n",
    "\n",
    "        if len(aspects) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term Polarity Intensity'.split(): tmp[k]=NULL_TOKEN\n",
    "            aspects = [tmp]\n",
    "        if len(holders) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term'.split(): tmp[k]=NULL_TOKEN\n",
    "            holders = [tmp]\n",
    "        if len(opinions) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term'.split(): tmp[k]=NULL_TOKEN\n",
    "            opinions = [tmp]\n",
    "\n",
    "        data['aspects'] = aspects\n",
    "        data['holder'] = holders\n",
    "        data['opinions'] = opinions\n",
    "        final_data.append(data)\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:00<00:00, 77902.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/dev_convert.json\n",
      "\n",
      "../../final_data/darmstadt_unis/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:00<00:00, 318023.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/test_convert.json\n",
      "\n",
      "../../final_data/darmstadt_unis/t1.json\n",
      "450.6 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2253/2253 [00:00<00:00, 140784.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/train_convert.json\n",
      "\n",
      "\n",
      "../../final_data/mpqa/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [00:00<00:00, 32735.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/dev_convert.json\n",
      "\n",
      "../../final_data/mpqa/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2112/2112 [00:00<00:00, 192043.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/test_convert.json\n",
      "\n",
      "../../final_data/mpqa/t1.json\n",
      "587.3000000000001 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5873/5873 [00:00<00:00, 122326.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/train_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "../../final_data/multibooked_ca/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 33504.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/dev_convert.json\n",
      "\n",
      "../../final_data/multibooked_ca/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 335/335 [00:00<00:00, 167512.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/test_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../../final_data/multibooked_ca/t1.json\n",
      "176.1 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1174/1174 [00:00<00:00, 36679.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/train_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "../../final_data/multibooked_eu/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 50670.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/dev_convert.json\n",
      "\n",
      "../../final_data/multibooked_eu/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305/305 [00:00<00:00, 305022.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/test_convert.json\n",
      "\n",
      "../../final_data/multibooked_eu/t1.json\n",
      "74.41000000000001 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:00<00:00, 39371.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/train_convert.json\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../../final_data/norec/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1531/1531 [00:00<00:00, 54662.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/dev_convert.json\n",
      "\n",
      "../../final_data/norec/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1272/1272 [00:00<00:00, 42384.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/test_convert.json\n",
      "\n",
      "../../final_data/norec/t1.json\n",
      "2590.2 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8634/8634 [00:00<00:00, 57546.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/train_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "../../final_data/opener_en/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:00<00:00, 41493.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/dev_convert.json\n",
      "\n",
      "../../final_data/opener_en/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:00<00:00, 166663.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/test_convert.json\n",
      "\n",
      "../../final_data/opener_en/t1.json\n",
      "0 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1744/1744 [00:00<00:00, 41521.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/train_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "../../final_data/opener_es/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [00:00<00:00, 29419.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/dev_convert.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410/410 [00:00<00:00, 204868.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/test_convert.json\n",
      "\n",
      "../../final_data/opener_es/t1.json\n",
      "86.28 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1438/1438 [00:00<00:00, 15624.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/train_convert.json\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_dict = {\n",
    "    'mpqa': 0.1,\n",
    "    'darmstadt_unis': 0.2,\n",
    "    'norec': 0.3,\n",
    "    'opener_en': 0,\n",
    "    'multibooked_eu': 0.07,\n",
    "    'multibooked_eu_crosslingual': 0.07,\n",
    "    'multibooked_ca': 0.15,\n",
    "    'opener_es': 0.06,\n",
    "    \"eng_combined\": 0.1\n",
    "}\n",
    "\n",
    "for folder in os.listdir('../../final_data'):\n",
    "    for file in os.listdir(f'../../final_data/{folder}'):\n",
    "        if '1.' not in file: continue\n",
    "        with open(f'../../final_data/{folder}/{file}', encoding=\"utf8\") as f: data = json.load(f)\n",
    "        print(f'../../final_data/{folder}/{file}')\n",
    "\n",
    "        data = format_convert(data, file, null_dict[folder])\n",
    "        if 't1' in file: dest = 'train_convert.json'\n",
    "        elif 'd1' in file: dest = 'dev_convert.json'\n",
    "        else: dest = 'test_convert.json'\n",
    "        print(f'../../final_data/{folder}/{dest}')\n",
    "        with open(f'../../final_data/{folder}/{dest}', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
