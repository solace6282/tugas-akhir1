{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import loads, dumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_words</th>\n",
       "      <th>words</th>\n",
       "      <th>aspects</th>\n",
       "      <th>holder</th>\n",
       "      <th>opinions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20020415/21.31.55-5725-0</td>\n",
       "      <td>NA Two IDF soldiers were lightly wounded in gu...</td>\n",
       "      <td>[NA, Two, IDF, soldiers, were, lightly, wounde...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20020415/21.31.55-5725-1</td>\n",
       "      <td>NA They were evacuated to Jerusalem 's Hadassa...</td>\n",
       "      <td>[NA, They, were, evacuated, to, Jerusalem, 's,...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20020415/21.31.55-5725-2</td>\n",
       "      <td>NA Sources in Bethlehem reported that the IDF ...</td>\n",
       "      <td>[NA, Sources, in, Bethlehem, reported, that, t...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20020415/21.31.55-5725-3</td>\n",
       "      <td>NA The Palestinians also reported that the IDF...</td>\n",
       "      <td>[NA, The, Palestinians, also, reported, that, ...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20020509/22.11.01-7259-0</td>\n",
       "      <td>NA Shanghai , May 9 ( XINHUA )--</td>\n",
       "      <td>[NA, Shanghai, ,, May, 9, (, XINHUA, )--]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>20011112/20.33.31-29984-10</td>\n",
       "      <td>NA And it is possible that , at some given tim...</td>\n",
       "      <td>[NA, And, it, is, possible, that, ,, at, some,...</td>\n",
       "      <td>[{'index': 0, 'from': 15, 'to': 16, 'term': 'i...</td>\n",
       "      <td>[{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]</td>\n",
       "      <td>[{'index': 0, 'from': 14, 'to': 18, 'term': 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>20011112/20.33.31-29984-13</td>\n",
       "      <td>NA \" The US authorities would not like to have...</td>\n",
       "      <td>[NA, \", The, US, authorities, would, not, like...</td>\n",
       "      <td>[{'index': 0, 'from': 9, 'to': 28, 'term': 'ha...</td>\n",
       "      <td>[{'index': 0, 'from': 30, 'to': 31, 'term': 'h...</td>\n",
       "      <td>[{'index': 0, 'from': 5, 'to': 8, 'term': 'wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>20011112/20.33.31-29984-21</td>\n",
       "      <td>NA He emphasized : \" Now Mexico is positioned ...</td>\n",
       "      <td>[NA, He, emphasized, :, \", Now, Mexico, is, po...</td>\n",
       "      <td>[{'index': 0, 'from': 31, 'to': 34, 'term': 't...</td>\n",
       "      <td>[{'index': 0, 'from': 1, 'to': 2, 'term': 'He'}]</td>\n",
       "      <td>[{'index': 0, 'from': 26, 'to': 31, 'term': 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>20011112/20.33.31-29984-22</td>\n",
       "      <td>NA In terms of market , the Mexican exports do...</td>\n",
       "      <td>[NA, In, terms, of, market, ,, the, Mexican, e...</td>\n",
       "      <td>[{'index': 0, 'from': 20, 'to': 21, 'term': 'O...</td>\n",
       "      <td>[{'index': 0, 'from': 25, 'to': 32, 'term': 'M...</td>\n",
       "      <td>[{'index': 0, 'from': 17, 'to': 19, 'term': 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>20020320/21.03.16-25474-6</td>\n",
       "      <td>NA The army would like to request whoever is e...</td>\n",
       "      <td>[NA, The, army, would, like, to, request, whoe...</td>\n",
       "      <td>[{'index': 0, 'from': 7, 'to': 14, 'term': 'wh...</td>\n",
       "      <td>[{'index': 0, 'from': 19, 'to': 20, 'term': 'h...</td>\n",
       "      <td>[{'index': 0, 'from': 3, 'to': 7, 'term': 'wou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1819 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sent_id  \\\n",
       "0       20020415/21.31.55-5725-0   \n",
       "1       20020415/21.31.55-5725-1   \n",
       "2       20020415/21.31.55-5725-2   \n",
       "3       20020415/21.31.55-5725-3   \n",
       "4       20020509/22.11.01-7259-0   \n",
       "...                          ...   \n",
       "1814  20011112/20.33.31-29984-10   \n",
       "1815  20011112/20.33.31-29984-13   \n",
       "1816  20011112/20.33.31-29984-21   \n",
       "1817  20011112/20.33.31-29984-22   \n",
       "1818   20020320/21.03.16-25474-6   \n",
       "\n",
       "                                              raw_words  \\\n",
       "0     NA Two IDF soldiers were lightly wounded in gu...   \n",
       "1     NA They were evacuated to Jerusalem 's Hadassa...   \n",
       "2     NA Sources in Bethlehem reported that the IDF ...   \n",
       "3     NA The Palestinians also reported that the IDF...   \n",
       "4                      NA Shanghai , May 9 ( XINHUA )--   \n",
       "...                                                 ...   \n",
       "1814  NA And it is possible that , at some given tim...   \n",
       "1815  NA \" The US authorities would not like to have...   \n",
       "1816  NA He emphasized : \" Now Mexico is positioned ...   \n",
       "1817  NA In terms of market , the Mexican exports do...   \n",
       "1818  NA The army would like to request whoever is e...   \n",
       "\n",
       "                                                  words  \\\n",
       "0     [NA, Two, IDF, soldiers, were, lightly, wounde...   \n",
       "1     [NA, They, were, evacuated, to, Jerusalem, 's,...   \n",
       "2     [NA, Sources, in, Bethlehem, reported, that, t...   \n",
       "3     [NA, The, Palestinians, also, reported, that, ...   \n",
       "4             [NA, Shanghai, ,, May, 9, (, XINHUA, )--]   \n",
       "...                                                 ...   \n",
       "1814  [NA, And, it, is, possible, that, ,, at, some,...   \n",
       "1815  [NA, \", The, US, authorities, would, not, like...   \n",
       "1816  [NA, He, emphasized, :, \", Now, Mexico, is, po...   \n",
       "1817  [NA, In, terms, of, market, ,, the, Mexican, e...   \n",
       "1818  [NA, The, army, would, like, to, request, whoe...   \n",
       "\n",
       "                                                aspects  \\\n",
       "0     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "1     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "2     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "3     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "4     [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'...   \n",
       "...                                                 ...   \n",
       "1814  [{'index': 0, 'from': 15, 'to': 16, 'term': 'i...   \n",
       "1815  [{'index': 0, 'from': 9, 'to': 28, 'term': 'ha...   \n",
       "1816  [{'index': 0, 'from': 31, 'to': 34, 'term': 't...   \n",
       "1817  [{'index': 0, 'from': 20, 'to': 21, 'term': 'O...   \n",
       "1818  [{'index': 0, 'from': 7, 'to': 14, 'term': 'wh...   \n",
       "\n",
       "                                                 holder  \\\n",
       "0      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "1      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "2      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "3      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "4      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "...                                                 ...   \n",
       "1814   [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]   \n",
       "1815  [{'index': 0, 'from': 30, 'to': 31, 'term': 'h...   \n",
       "1816   [{'index': 0, 'from': 1, 'to': 2, 'term': 'He'}]   \n",
       "1817  [{'index': 0, 'from': 25, 'to': 32, 'term': 'M...   \n",
       "1818  [{'index': 0, 'from': 19, 'to': 20, 'term': 'h...   \n",
       "\n",
       "                                               opinions  \n",
       "0      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "1      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "2      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "3      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "4      [{'index': 0, 'from': 0, 'to': 1, 'term': 'NA'}]  \n",
       "...                                                 ...  \n",
       "1814  [{'index': 0, 'from': 14, 'to': 18, 'term': 'h...  \n",
       "1815  [{'index': 0, 'from': 5, 'to': 8, 'term': 'wou...  \n",
       "1816  [{'index': 0, 'from': 26, 'to': 31, 'term': 't...  \n",
       "1817  [{'index': 0, 'from': 17, 'to': 19, 'term': 't...  \n",
       "1818  [{'index': 0, 'from': 3, 'to': 7, 'term': 'wou...  \n",
       "\n",
       "[1819 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = ['mpqa']\n",
    "\n",
    "df1 = [pd.read_json(f\"../../final_data/{i}/train_convert.json\") for i in datas]\n",
    "df = pd.concat(df1)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('ddx.json')\n",
    "df2 = pd.read_json('ddx1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>raw_words</th>\n",
       "      <th>words</th>\n",
       "      <th>aspects</th>\n",
       "      <th>holder</th>\n",
       "      <th>opinions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent_id, raw_words, words, aspects, holder, opinions]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1[\"opinions\"] == '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'ddx1.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, json\n",
    "\n",
    "NULL_TOKEN = 'NA'\n",
    "START_WORD = 'NA '\n",
    "NULL_INDEX = 0\n",
    "\n",
    "def char2word_offset(sent, start_end_str):\n",
    "    start, end = [int(i) for i in start_end_str.split(':')]\n",
    "    start, end = start+len(START_WORD), end+len(START_WORD)\n",
    "    word_start, word_end = None, None\n",
    "    space_count = 0\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i]==' ': space_count += 1\n",
    "        if i>=start and word_start is None: word_start = space_count\n",
    "        if i>=end and word_start is not None: word_end = space_count; break\n",
    "    if word_end is None and word_start is not None: word_end = space_count+1\n",
    "    assert (type(word_start) == type(word_end))\n",
    "    if word_start is None: word_start, word_end = NULL_INDEX, NULL_INDEX+1\n",
    "    return int(word_start), int(word_end)\n",
    "\n",
    "def format_convert(original_data, file, threshold=0):\n",
    "    final_data = []\n",
    "    blank_cnt = 0\n",
    "    threshold *= len(original_data)\n",
    "    if 't1' in file: print(f'{threshold} limit for null opinions')\n",
    "\n",
    "    for row in tqdm(original_data):\n",
    "        data = {}\n",
    "        if 't1' in file:\n",
    "            if blank_cnt <= threshold and len(row['opinions'])==0: blank_cnt += 1\n",
    "            elif len(row['opinions'])==0: continue\n",
    "\n",
    "        row['text'] = START_WORD + row['text']\n",
    "        data['sent_id'] = row['sent_id']\n",
    "        data['raw_words'] = row['text']\n",
    "        data['words'] = data['raw_words'].split()\n",
    "        aspects, holders, opinions = [], [], []\n",
    "\n",
    "        for cnt, original_opinion in enumerate(row['opinions']):\n",
    "            aspect, holder, opinion = {}, {}, {}\n",
    "            aspect['index'], holder['index'], opinion['index'] = cnt, cnt, cnt\n",
    "\n",
    "            ############################################# Aspect #############################################\n",
    "            try:aspect['from'], aspect['to'] = char2word_offset(data['raw_words'], original_opinion['Target'][1][0])\n",
    "            except: aspect['from'], aspect['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if aspect['from'] == aspect['to']-1 == NULL_INDEX:\n",
    "                aspect['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                aspect['term'] = original_opinion['Target'][0][0]\n",
    "\n",
    "            try: aspect['Polarity'] = str(original_opinion['Polarity'])\n",
    "            except: aspect['Polarity'] = NULL_TOKEN; print(original_opinion)\n",
    "            try: aspect['Intensity'] = str(original_opinion['Intensity'])\n",
    "            except: aspect['Intensity'] = NULL_TOKEN; print(original_opinion)\n",
    "\n",
    "            ############################################# Holder #############################################\n",
    "            try:holder['from'], holder['to'] = char2word_offset(data['raw_words'], original_opinion['Source'][1][0])\n",
    "            except: holder['from'], holder['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if holder['from'] == holder['to']-1 == NULL_INDEX:\n",
    "                holder['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                holder['term']  = original_opinion['Source'][0][0]\n",
    "\n",
    "            ############################################# Opinion #############################################\n",
    "            try:opinion['from'], opinion['to'] = char2word_offset(data['raw_words'], original_opinion['Polar_expression'][1][0])\n",
    "            except: opinion['from'], opinion['to'] = NULL_INDEX, NULL_INDEX+1\n",
    "            if opinion['from'] == opinion['to']-1 == NULL_INDEX:\n",
    "                opinion['term'] = NULL_TOKEN\n",
    "            else:\n",
    "                opinion['term'] = original_opinion['Polar_expression'][0][0]\n",
    "\n",
    "            aspects.append(aspect)\n",
    "            holders.append(holder)\n",
    "            opinions.append(opinion)\n",
    "\n",
    "        if len(aspects) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term Polarity Intensity'.split(): tmp[k]=NULL_TOKEN\n",
    "            aspects = [tmp]\n",
    "        if len(holders) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term'.split(): tmp[k]=NULL_TOKEN\n",
    "            holders = [tmp]\n",
    "        if len(opinions) == 0:\n",
    "            tmp = {}; tmp['index'] = 0; tmp['from']=NULL_INDEX; tmp['to']=NULL_INDEX+1\n",
    "            for k in 'term'.split(): tmp[k]=NULL_TOKEN\n",
    "            opinions = [tmp]\n",
    "\n",
    "        data['aspects'] = aspects\n",
    "        data['holder'] = holders\n",
    "        data['opinions'] = opinions\n",
    "        final_data.append(data)\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:00<00:00, 108071.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/dev_convert.json\n",
      "\n",
      "../../final_data/darmstadt_unis/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 318/318 [00:00<00:00, 79510.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/test_convert.json\n",
      "\n",
      "../../final_data/darmstadt_unis/t1.json\n",
      "450.6 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2253/2253 [00:00<00:00, 140772.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/darmstadt_unis/train_convert.json\n",
      "\n",
      "\n",
      "../../final_data/mpqa/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1997/1997 [00:00<00:00, 26979.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/dev_convert.json\n",
      "\n",
      "../../final_data/mpqa/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2055/2055 [00:00<00:00, 93408.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/test_convert.json\n",
      "\n",
      "../../final_data/mpqa/t1.json\n",
      "564.3000000000001 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5643/5643 [00:00<00:00, 120029.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/mpqa/train_convert.json\n",
      "\n",
      "\n",
      "../../final_data/multibooked_ca/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 167/167 [00:00<00:00, 41730.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/dev_convert.json\n",
      "\n",
      "../../final_data/multibooked_ca/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335/335 [00:00<00:00, 33496.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/test_convert.json\n",
      "\n",
      "../../final_data/multibooked_ca/t1.json\n",
      "176.1 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1174/1174 [00:00<00:00, 37867.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_ca/train_convert.json\n",
      "\n",
      "\n",
      "../../final_data/multibooked_eu/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [00:00<00:00, 37971.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/dev_convert.json\n",
      "\n",
      "../../final_data/multibooked_eu/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 305/305 [00:00<00:00, 50845.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/test_convert.json\n",
      "\n",
      "../../final_data/multibooked_eu/t1.json\n",
      "74.41000000000001 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1063/1063 [00:00<00:00, 44271.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/multibooked_eu/train_convert.json\n",
      "\n",
      "\n",
      "\n",
      "../../final_data/norec/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1531/1531 [00:00<00:00, 19879.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/dev_convert.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1272/1272 [00:00<00:00, 55293.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/test_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../../final_data/norec/t1.json\n",
      "2590.2 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8634/8634 [00:00<00:00, 35299.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/norec/train_convert.json\n",
      "\n",
      "\n",
      "../../final_data/opener_en/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 35607.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/dev_convert.json\n",
      "\n",
      "../../final_data/opener_en/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 35645.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/test_convert.json\n",
      "\n",
      "../../final_data/opener_en/t1.json\n",
      "0 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1744/1744 [00:00<00:00, 43576.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_en/train_convert.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "../../final_data/opener_es/d1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [00:00<00:00, 29411.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/dev_convert.json\n",
      "\n",
      "../../final_data/opener_es/f1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [00:00<00:00, 24715.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/test_convert.json\n",
      "\n",
      "../../final_data/opener_es/t1.json\n",
      "86.28 limit for null opinions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1438/1438 [00:00<00:00, 28753.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../final_data/opener_es/train_convert.json\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_dict = {\n",
    "    'mpqa': 0.1,\n",
    "    'darmstadt_unis': 0.2,\n",
    "    'norec': 0.3,\n",
    "    'opener_en': 0,\n",
    "    'multibooked_eu': 0.07,\n",
    "    'multibooked_eu_crosslingual': 0.07,\n",
    "    'multibooked_ca': 0.15,\n",
    "    'opener_es': 0.06,\n",
    "    \"eng_combined\": 0.1\n",
    "}\n",
    "\n",
    "for folder in os.listdir('../../final_data'):\n",
    "    for file in os.listdir(f'../../final_data/{folder}'):\n",
    "        if '1.' not in file: continue\n",
    "        with open(f'../../final_data/{folder}/{file}', encoding=\"utf8\") as f: data = json.load(f)\n",
    "\n",
    "        data = format_convert(data, file, null_dict[folder])\n",
    "        if 't1' in file: dest = 'train_convert.json'\n",
    "        elif 'd1' in file: dest = 'dev_convert.json'\n",
    "        elif 'f1' in file: dest = 'test_convert.json'\n",
    "        print(f'../../final_data/{folder}/{dest}')\n",
    "        with open(f'../../final_data/{folder}/{dest}', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
